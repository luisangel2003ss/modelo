{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luisangel2003ss/modelo/blob/main/cpyWaterSpillOil.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZNSyLFOpLPm"
      },
      "source": [
        "# Notebook: Predicción multitarea con Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXCudcYqpgh4"
      },
      "source": [
        "# Librerías"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I32VqEN3pGR0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import base64\n",
        "import requests\n",
        "import re\n",
        "import joblib\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n",
        "from tensorflow.keras.regularizers import l2\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "print(os.path.getsize(\"spill_data_cleaned.csv\"))  # Debe mostrar tamaño > 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"spill_data_cleaned.csv\", sep=';', encoding='latin-1', on_bad_lines='skip', engine='python')\n",
        "\n",
        "# Convertir a minúsculas solo las columnas de texto que existen\n",
        "for col in df.select_dtypes(include='object').columns:\n",
        "    df[col] = df[col].astype(str).str.lower()\n",
        "\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShnuwZONp1zZ"
      },
      "source": [
        "# 1. FUNCION PARA ENTRENAR CON OPTUNA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15oBqomOp5cY"
      },
      "outputs": [],
      "source": [
        "def run_optuna_optimization(X_train, y_reg_train, y_clf_train, n_trials=150):\n",
        "    \"\"\"\n",
        "    Ejecuta optimización con Optuna y guarda los mejores parámetros\n",
        "    ESTO SE EJECUTA SOLO UNA VEZ PARA ENCONTRAR LOS MEJORES HIPERPARÁMETROS\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"EJECUTANDO OPTIMIZACIÓN CON OPTUNA...\")\n",
        "    print(\"Esto puede tomar tiempo pero solo se hace UNA VEZ\")\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    num_classes = y_clf_train.shape[1]\n",
        "\n",
        "    # División para validación interna\n",
        "    X_train_opt, X_val_opt, y_reg_train_opt, y_reg_val_opt, y_clf_train_opt, y_clf_val_opt = train_test_split(\n",
        "        X_train, y_reg_train, y_clf_train, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    def objective(trial):\n",
        "\n",
        "        # Hiperparámetros a optimizar\n",
        "        n_layers = trial.suggest_int('n_layers', 2, 4)\n",
        "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
        "        l2_reg = trial.suggest_float('l2_reg', 1e-6, 1e-2, log=True)\n",
        "        learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
        "        optimizer_name = trial.suggest_categorical('optimizer', ['adam', 'rmsprop'])\n",
        "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
        "        use_batch_norm = trial.suggest_categorical('use_batch_norm', [True, False])\n",
        "\n",
        "        # Construir modelo\n",
        "        input_layer = Input(shape=(input_dim,))\n",
        "        x = input_layer\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            if i == 0:\n",
        "                neurons = trial.suggest_int(f'neurons_layer_{i}', 64, 256)\n",
        "            else:\n",
        "                max_neurons = max(32, int(neurons * 0.7))\n",
        "                neurons = trial.suggest_int(f'neurons_layer_{i}', 32, max_neurons)\n",
        "\n",
        "            x = Dense(neurons, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
        "\n",
        "            if use_batch_norm:\n",
        "                x = BatchNormalization()(x)\n",
        "\n",
        "            x = Dropout(dropout_rate)(x)\n",
        "\n",
        "        # Salidas\n",
        "        regression_output = Dense(1, name='regression')(x)\n",
        "        classification_output = Dense(num_classes, activation='softmax', name='classification')(x)\n",
        "\n",
        "        model = Model(inputs=input_layer, outputs=[regression_output, classification_output])\n",
        "\n",
        "        # Configurar optimizador\n",
        "        if optimizer_name == 'adam':\n",
        "            optimizer = Adam(learning_rate=learning_rate)\n",
        "        else:\n",
        "            optimizer = RMSprop(learning_rate=learning_rate)\n",
        "\n",
        "        model.compile(\n",
        "            optimizer=optimizer,\n",
        "            loss={'regression': 'mae', 'classification': 'categorical_crossentropy'},\n",
        "            loss_weights={'regression': 0.5, 'classification': 0.5}\n",
        "        )\n",
        "\n",
        "        # Entrenar\n",
        "        try:\n",
        "            history = model.fit(\n",
        "                X_train_opt,\n",
        "                {'regression': y_reg_train_opt, 'classification': y_clf_train_opt},\n",
        "                validation_data=(X_val_opt, {'regression': y_reg_val_opt, 'classification': y_clf_val_opt}),\n",
        "                epochs=35,\n",
        "                batch_size=batch_size,\n",
        "                callbacks=[EarlyStopping(patience=5, restore_best_weights=True)],\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            # Evaluar\n",
        "            val_results = model.evaluate(\n",
        "                X_val_opt,\n",
        "                {'regression': y_reg_val_opt, 'classification': y_clf_val_opt},\n",
        "                verbose=0\n",
        "            )\n",
        "\n",
        "            return val_results[0]  # Pérdida total\n",
        "\n",
        "        except Exception as e:\n",
        "            return float('inf')\n",
        "\n",
        "    # Ejecutar optimización\n",
        "    study = optuna.create_study(direction='minimize')\n",
        "    study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "    # Guardar mejores parámetros\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    best_params_data = {\n",
        "        'timestamp': timestamp,\n",
        "        'best_value': study.best_value,\n",
        "        'best_params': study.best_params,\n",
        "        'n_trials': len(study.trials),\n",
        "        'optimization_complete': True\n",
        "    }\n",
        "\n",
        "    filename = f'best_params_{timestamp}.json'\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(best_params_data, f, indent=2)\n",
        "\n",
        "    print(f\"Optimización completada!\")\n",
        "    print(f\"Mejores parámetros guardados en: {filename}\")\n",
        "    print(f\"Mejor pérdida: {study.best_value:.4f}\")\n",
        "\n",
        "    return filename, study.best_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTySuvEaqrj4"
      },
      "source": [
        "# 2. FUNCIÓN PARA CARGAR PARÁMETROS DESDE JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5h_y3dXqs8x"
      },
      "outputs": [],
      "source": [
        "def load_best_params_from_json(json_filename):\n",
        "    \"\"\"\n",
        "    Carga los mejores parámetros desde un archivo JSON\n",
        "    ESTO SE USA EN PRODUCCIÓN - NO NECESITA REOPTIMIZAR\n",
        "    \"\"\"\n",
        "\n",
        "    if not os.path.exists(json_filename):\n",
        "        raise FileNotFoundError(f\"Archivo no encontrado: {json_filename}\")\n",
        "\n",
        "    with open(json_filename, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    print(f\"Cargando parámetros desde: {json_filename}\")\n",
        "    print(f\"Optimización realizada: {data['timestamp']}\")\n",
        "    print(f\"Mejor pérdida obtenida: {data['best_value']:.4f}\")\n",
        "\n",
        "    return data['best_params']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmerSgrTsJsn"
      },
      "source": [
        "# 3. FUNCIÓN PARA CONSTRUIR MODELO CON PARÁMETROS DADOS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwNM9WHcsOwB"
      },
      "outputs": [],
      "source": [
        "def build_model_from_params(best_params, input_dim, num_classes):\n",
        "    # Extraer parámetros con valores por defecto si no existen\n",
        "    n_layers = best_params.get('n_layers', 2)\n",
        "    units = best_params.get('units', 64)\n",
        "    dropout_rate = best_params.get('dropout_rate', 0.3)\n",
        "    l2_reg = best_params.get('l2_reg', 1e-4)\n",
        "    learning_rate = best_params.get('learning_rate', 1e-3)\n",
        "\n",
        "    # Construcción del modelo\n",
        "    inputs = Input(shape=(input_dim,), name=\"input_layer_1\") \n",
        "    x = inputs\n",
        "\n",
        "    for _ in range(n_layers):\n",
        "        x = Dense(units, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(l2_reg))(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Dropout(best_params.get('dropout_rate', 0.3))(x)\n",
        "        \n",
        "    # Salidas\n",
        "    regression_output = Dense(1, name='regression')(x)\n",
        "    classification_output = Dense(num_classes, activation='softmax', name='classification')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[regression_output, classification_output])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "        loss={\n",
        "            'regression': 'mse',\n",
        "            'classification': 'categorical_crossentropy'\n",
        "        },\n",
        "        metrics={\n",
        "            'regression': ['mae'],\n",
        "            'classification': ['accuracy']\n",
        "        }\n",
        "    )\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhWMrvwgvnLu"
      },
      "source": [
        "# 4. WORKFLOW PRINCIPAL, OPTIMIZAR O USAR PARÁMETROS EXISTENTES?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qOjCMBsjbOJU",
        "outputId": "de484283-22d2-4a98-cb28-b77d5eb7faf7"
      },
      "outputs": [],
      "source": [
        "# ===== CONFIGURACIÓN =====\n",
        "GITHUB_REPO = \"luisangel2003ss/modelo\"  # Tu repositorio\n",
        "GITHUB_BRANCH = \"main\"  # Rama donde subir\n",
        "\n",
        "# ===== FUNCIONES CORREGIDAS =====\n",
        "def select_best_params_file(json_files):\n",
        "    \"\"\"\n",
        "    Selecciona automáticamente el mejor archivo de parámetros basado en:\n",
        "    1. Fecha de modificación más reciente\n",
        "    2. Si hay archivos con métricas en el nombre, usa el de mejores métricas\n",
        "    \"\"\"\n",
        "    if not json_files:\n",
        "        print(\"⚠️ No se encontraron archivos de parámetros\")\n",
        "        return None\n",
        "\n",
        "    print(f\"🔍 Se encontraron {len(json_files)} archivos de parámetros:\")\n",
        "\n",
        "    files_with_info = []\n",
        "    for f in json_files:\n",
        "        try:\n",
        "            modification_time = os.path.getmtime(f)\n",
        "            file_size = os.path.getsize(f)\n",
        "\n",
        "            metrics_in_name = None\n",
        "            if 'r2_' in f.lower() or 'acc_' in f.lower():\n",
        "                numbers = re.findall(r'[\\d.]+', f)\n",
        "                if numbers:\n",
        "                    try:\n",
        "                        metrics_in_name = float(numbers[0])\n",
        "                    except ValueError:\n",
        "                        metrics_in_name = None\n",
        "\n",
        "            files_with_info.append({\n",
        "                'filename': f,\n",
        "                'mod_time': modification_time,\n",
        "                'metrics': metrics_in_name,\n",
        "                'size': file_size\n",
        "            })\n",
        "\n",
        "            mod_time_str = pd.to_datetime(modification_time, unit='s').strftime('%Y-%m-%d %H:%M')\n",
        "            metric_str = f\", métrica: {metrics_in_name}\" if metrics_in_name else \"\"\n",
        "            print(f\"  📄 {f} (modificado: {mod_time_str}, tamaño: {file_size} bytes{metric_str})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️ Error leyendo {f}: {e}\")\n",
        "            files_with_info.append({\n",
        "                'filename': f,\n",
        "                'mod_time': 0,\n",
        "                'metrics': None,\n",
        "                'size': 0\n",
        "            })\n",
        "\n",
        "    valid_files = [f for f in files_with_info if f['size'] > 0]\n",
        "\n",
        "    if not valid_files:\n",
        "        print(\"❌ No se encontraron archivos válidos\")\n",
        "        return None\n",
        "\n",
        "    files_with_metrics = [f for f in valid_files if f['metrics'] is not None]\n",
        "    if files_with_metrics:\n",
        "        best_file = max(files_with_metrics, key=lambda x: x['metrics'])\n",
        "        print(f\"✅ Seleccionado por mejores métricas: {best_file['filename']} (métrica: {best_file['metrics']})\")\n",
        "    else:\n",
        "        best_file = max(valid_files, key=lambda x: x['mod_time'])\n",
        "        mod_time_str = pd.to_datetime(best_file['mod_time'], unit='s').strftime('%Y-%m-%d %H:%M')\n",
        "        print(f\"✅ Seleccionado por fecha más reciente: {best_file['filename']} ({mod_time_str})\")\n",
        "\n",
        "    return best_file['filename']\n",
        "\n",
        "def load_and_compare_all_params(json_files):\n",
        "    \"\"\"\n",
        "    Carga todos los archivos de parámetros y selecciona el mejor basado en métricas guardadas\n",
        "    \"\"\"\n",
        "    if not json_files:\n",
        "        print(\"⚠️ No se encontraron archivos de parámetros\")\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\n🔄 Analizando {len(json_files)} archivos de parámetros...\")\n",
        "    best_params = None\n",
        "    best_file = None\n",
        "    best_score = -float('inf')\n",
        "    valid_files_count = 0\n",
        "\n",
        "    for filename in json_files:\n",
        "        try:\n",
        "            if not os.path.exists(filename) or os.path.getsize(filename) == 0:\n",
        "                print(f\"  ⚠️ Archivo vacío o inexistente: {filename}\")\n",
        "                continue\n",
        "\n",
        "            with open(filename, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            if not data:\n",
        "                print(f\"  ⚠️ Archivo JSON vacío: {filename}\")\n",
        "                continue\n",
        "\n",
        "            valid_files_count += 1\n",
        "\n",
        "            score = 0\n",
        "            score_source = \"timestamp\"\n",
        "\n",
        "            if 'metrics' in data and isinstance(data['metrics'], dict):\n",
        "                r2 = data['metrics'].get('r2', 0)\n",
        "                accuracy = data['metrics'].get('accuracy', 0)\n",
        "\n",
        "                if isinstance(r2, (int, float)) and isinstance(accuracy, (int, float)):\n",
        "                    score = r2 * 0.6 + accuracy * 0.4\n",
        "                    score_source = f\"métricas (R2:{r2:.3f}, Acc:{accuracy:.3f})\"\n",
        "                else:\n",
        "                    score = os.path.getmtime(filename)\n",
        "                    score_source = \"timestamp (métricas inválidas)\"\n",
        "\n",
        "            elif 'best_value' in data and isinstance(data['best_value'], (int, float)):\n",
        "                score = -data['best_value']\n",
        "                score_source = f\"Optuna best_value ({data['best_value']})\"\n",
        "            else:\n",
        "                score = os.path.getmtime(filename)\n",
        "                score_source = \"timestamp (fallback)\"\n",
        "\n",
        "            print(f\"  📊 {filename}: score={score:.4f} ({score_source})\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                if 'best_params' in data:\n",
        "                    best_params = data['best_params']\n",
        "                elif 'model_params' in data:\n",
        "                    best_params = data['model_params']\n",
        "                else:\n",
        "                    best_params = data\n",
        "                best_file = filename\n",
        "\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"  ❌ Error JSON en {filename}: {e}\")\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error cargando {filename}: {e}\")\n",
        "\n",
        "    if best_file and best_params:\n",
        "        print(f\"✅ Mejor archivo seleccionado: {best_file} (score: {best_score:.4f})\")\n",
        "        print(f\"📋 Parámetros cargados: {len(best_params)} elementos\")\n",
        "    else:\n",
        "        print(\"❌ No se pudo seleccionar ningún archivo válido\")\n",
        "\n",
        "    return best_file, best_params\n",
        "\n",
        "def save_metrics_with_timestamp(metrics_data, prefix=\"metrics\", include_score=True):\n",
        "    \"\"\"\n",
        "    Guarda las métricas en un archivo JSON con timestamp\n",
        "    \"\"\"\n",
        "    try:\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        filename = f\"{prefix}_{timestamp}\"\n",
        "\n",
        "        if include_score and 'metrics' in metrics_data:\n",
        "            metrics = metrics_data['metrics']\n",
        "            if 'r2' in metrics and 'accuracy' in metrics:\n",
        "                r2 = metrics['r2']\n",
        "                accuracy = metrics['accuracy']\n",
        "                score = r2 * 0.6 + accuracy * 0.4\n",
        "                filename += f\"_score_{score:.4f}\"\n",
        "\n",
        "        filename += \".json\"\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(metrics_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"💾 Métricas guardadas en: {filename}\")\n",
        "        return filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error guardando métricas: {e}\")\n",
        "        return None\n",
        "\n",
        "def main_workflow():\n",
        "    \"\"\"\n",
        "    Workflow principal automatizado\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"🚀 INICIANDO WORKFLOW AUTOMATIZADO DE OPTIMIZACIÓN\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    print(\"\\n📊 Preparando datos...\")\n",
        "\n",
        "    if not os.path.exists(\"spill_data_cleaned.csv\"):\n",
        "        print(\"❌ No se encontró el archivo 'spill_data_cleaned.csv'\")\n",
        "        print(\"   Asegúrate de que el archivo esté en el directorio actual\")\n",
        "        return None, None\n",
        "\n",
        "    df = pd.read_csv(\"spill_data_cleaned.csv\", sep=';', encoding='latin-1')\n",
        "    df = df.dropna(subset=['release_prod_water_edit'])\n",
        "    df['log_release_prod_water_edit'] = np.log1p(df['release_prod_water_edit'])\n",
        "\n",
        "    cat_cols = ['operator_edit', 'county_edit', 'type_operation', 'source', 'probable_cause_edit']\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna('unknown')\n",
        "\n",
        "    df['date'] = pd.to_datetime(df['date_of_spill_edit'])\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df.drop(columns=['date'], inplace=True)\n",
        "\n",
        "    y_reg = df['log_release_prod_water_edit']\n",
        "    y_clf = df['probable_cause_edit']\n",
        "    X = df[['year','month','operator_edit','county_edit','type_operation','source']]\n",
        "\n",
        "    num_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    cat_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ])\n",
        "\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "    le_causa = LabelEncoder()\n",
        "    y_clf_enc = le_causa.fit_transform(y_clf)\n",
        "    y_clf_oh = to_categorical(y_clf_enc)\n",
        "\n",
        "    X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(\n",
        "        X_processed, y_reg, y_clf_oh, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    num_classes = y_clf_oh.shape[1]\n",
        "\n",
        "    print(f\"✅ Datos preparados: {X_train.shape[0]} muestras de entrenamiento\")\n",
        "\n",
        "    json_files = [f for f in os.listdir('.') if f.startswith('best_params_') and f.endswith('.json')]\n",
        "\n",
        "    if json_files:\n",
        "        print(f\"\\n🔍 Archivos de parámetros encontrados: {len(json_files)}\")\n",
        "        best_file, best_params = load_and_compare_all_params(json_files)\n",
        "\n",
        "        if best_params:\n",
        "            print(f\"✅ Usando parámetros del archivo: {best_file}\")\n",
        "            print(\"⚠️ Necesitas implementar build_model_from_params()\")\n",
        "        else:\n",
        "            print(\"❌ Error cargando parámetros, necesitas ejecutar optimización...\")\n",
        "            print(\"⚠️ Necesitas implementar run_optuna_optimization()\")\n",
        "    else:\n",
        "        print(\"\\n🆕 No se encontraron archivos de parámetros - ejecutando optimización inicial...\")\n",
        "        print(\"⚠️ Necesitas implementar run_optuna_optimization()\")\n",
        "\n",
        "    print(\"\\n🎯 Simulando entrenamiento y evaluación...\")\n",
        "\n",
        "    r2 = 0.85\n",
        "    mse = 0.25\n",
        "    mae = 0.15\n",
        "    accuracy = 0.78\n",
        "\n",
        "    print(f\"📊 Métricas simuladas:\")\n",
        "    print(f\"   R² Score: {r2:.4f}\")\n",
        "    print(f\"   MSE: {mse:.4f}\")\n",
        "    print(f\"   MAE: {mae:.4f}\")\n",
        "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    metrics_summary = {\n",
        "        \"timestamp\": pd.Timestamp.now().isoformat(),\n",
        "        \"metrics\": {\n",
        "            \"r2\": r2,\n",
        "            \"mse\": mse,\n",
        "            \"mae\": mae,\n",
        "            \"accuracy\": accuracy\n",
        "        },\n",
        "        \"model_params\": best_params if 'best_params' in locals() else {},\n",
        "        \"training_epochs\": 93,\n",
        "        \"dataset_info\": {\n",
        "            \"train_samples\": X_train.shape[0],\n",
        "            \"test_samples\": X_test.shape[0],\n",
        "            \"features\": X_train.shape[1],\n",
        "            \"classes\": num_classes\n",
        "        }\n",
        "    }\n",
        "\n",
        "    local_filename = save_metrics_with_timestamp(metrics_summary, \"best_params\", True)\n",
        "\n",
        "    if local_filename:\n",
        "        print(f\"💾 Métricas guardadas localmente en: {local_filename}\")\n",
        "\n",
        "    print(\"\\n🎉 Workflow completado!\")\n",
        "    print(\"\\n📋 Próximos pasos:\")\n",
        "    print(\"   1. Integra tus funciones build_model_from_params() y run_optuna_optimization()\")\n",
        "    print(\"   2. Reemplaza las métricas simuladas con las reales de tu modelo\")\n",
        "\n",
        "    return None, metrics_summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🔧 Ejecutando workflow de optimización...\")\n",
        "\n",
        "    required_files = [\"spill_data_cleaned.csv\"]\n",
        "    missing_files = [f for f in required_files if not os.path.exists(f)]\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"❌ Archivos faltantes: {missing_files}\")\n",
        "        print(\"   Asegúrate de tener todos los archivos necesarios en el directorio\")\n",
        "    else:\n",
        "        model, metrics = main_workflow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_best_params_file(json_files):\n",
        "    \"\"\"\n",
        "    Selecciona automáticamente el mejor archivo de parámetros basado en:\n",
        "    1. Fecha de modificación más reciente\n",
        "    2. Si hay archivos con métricas en el nombre, usa el de mejores métricas\n",
        "    \"\"\"\n",
        "    if not json_files:\n",
        "        return None\n",
        "\n",
        "    print(f\"🔍 Se encontraron {len(json_files)} archivos de parámetros:\")\n",
        "\n",
        "    # Mostrar archivos disponibles\n",
        "    files_with_info = []\n",
        "    for f in json_files:\n",
        "        try:\n",
        "            # Obtener información del archivo\n",
        "            creation_time = os.path.getctime(f)\n",
        "            modification_time = os.path.getmtime(f)\n",
        "\n",
        "            # Intentar extraer métricas del nombre del archivo si las tiene\n",
        "            metrics_in_name = None\n",
        "            if 'r2_' in f.lower() or 'acc_' in f.lower():\n",
        "                # Extraer valores numéricos del nombre\n",
        "                import re\n",
        "                numbers = re.findall(r'[\\d.]+', f)\n",
        "                if numbers:\n",
        "                    metrics_in_name = float(numbers[0])\n",
        "\n",
        "            files_with_info.append({\n",
        "                'filename': f,\n",
        "                'mod_time': modification_time,\n",
        "                'metrics': metrics_in_name\n",
        "            })\n",
        "\n",
        "            print(f\"  📄 {f} (modificado: {pd.to_datetime(modification_time, unit='s').strftime('%Y-%m-%d %H:%M')})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ⚠️  Error leyendo {f}: {e}\")\n",
        "            files_with_info.append({\n",
        "                'filename': f,\n",
        "                'mod_time': 0,\n",
        "                'metrics': None\n",
        "            })\n",
        "\n",
        "    # Estrategia de selección:\n",
        "    # 1. Si hay archivos con métricas en el nombre, usar el de mejor métrica\n",
        "    # 2. Si no, usar el más reciente\n",
        "\n",
        "    files_with_metrics = [f for f in files_with_info if f['metrics'] is not None]\n",
        "\n",
        "    if files_with_metrics:\n",
        "        # Usar el archivo con mejores métricas (asumiendo que valores más altos son mejores)\n",
        "        best_file = max(files_with_metrics, key=lambda x: x['metrics'])\n",
        "        print(f\"✅ Seleccionado por mejores métricas: {best_file['filename']} (métrica: {best_file['metrics']})\")\n",
        "    else:\n",
        "        # Usar el más reciente\n",
        "        best_file = max(files_with_info, key=lambda x: x['mod_time'])\n",
        "        print(f\"✅ Seleccionado por fecha más reciente: {best_file['filename']}\")\n",
        "\n",
        "    return best_file['filename']\n",
        "\n",
        "def load_and_compare_all_params(json_files):\n",
        "    \"\"\"\n",
        "    Carga todos los archivos de parámetros y selecciona el mejor basado en métricas guardadas\n",
        "    \"\"\"\n",
        "    if not json_files:\n",
        "        return None, None\n",
        "\n",
        "    print(f\"\\n🔄 Analizando {len(json_files)} archivos de parámetros...\")\n",
        "\n",
        "    best_params = None\n",
        "    best_file = None\n",
        "    best_score = -float('inf')\n",
        "\n",
        "    for filename in json_files:\n",
        "        try:\n",
        "            with open(filename, 'r') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            # Buscar métricas de evaluación en el archivo\n",
        "            score = 0\n",
        "            if 'metrics' in data:\n",
        "                # Si hay métricas guardadas, usar R2 + Accuracy como score combinado\n",
        "                r2 = data['metrics'].get('r2', 0)\n",
        "                accuracy = data['metrics'].get('accuracy', 0)\n",
        "                score = r2 * 0.6 + accuracy * 0.4  # Peso 60% R2, 40% Accuracy\n",
        "            elif 'best_value' in data:\n",
        "                # Si hay best_value de Optuna\n",
        "                score = -data['best_value']  # Negativo porque Optuna minimiza\n",
        "            else:\n",
        "                # Usar timestamp como fallback\n",
        "                score = os.path.getmtime(filename)\n",
        "\n",
        "            print(f\"  📊 {filename}: score={score:.4f}\")\n",
        "\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = data.get('best_params', data)\n",
        "                best_file = filename\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ❌ Error cargando {filename}: {e}\")\n",
        "\n",
        "    if best_file:\n",
        "        print(f\"✅ Mejor archivo seleccionado: {best_file} (score: {best_score:.4f})\")\n",
        "\n",
        "    return best_file, best_params\n",
        "\n",
        "def main_workflow():\n",
        "    \"\"\"\n",
        "    Workflow principal automatizado - sin preguntas interactivas\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"🚀 INICIANDO WORKFLOW AUTOMATIZADO DE OPTIMIZACIÓN\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # Cargar y preparar datos (tu código existente)\n",
        "    print(\"\\n📊 Preparando datos...\")\n",
        "    df = pd.read_csv(\"spill_data_cleaned.csv\", sep=';', encoding='latin-1')\n",
        "    df = df.dropna(subset=['release_prod_water_edit'])\n",
        "    df['log_release_prod_water_edit'] = np.log1p(df['release_prod_water_edit'])\n",
        "\n",
        "    # Preparar características\n",
        "    cat_cols = ['operator_edit', 'county_edit', 'type_operation', 'source', 'probable_cause_edit']\n",
        "    for col in cat_cols:\n",
        "        df[col] = df[col].fillna('unknown')\n",
        "\n",
        "    df['date'] = pd.to_datetime(df['date_of_spill_edit'])\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df.drop(columns=['date'], inplace=True)\n",
        "\n",
        "    y_reg = df['log_release_prod_water_edit']\n",
        "    y_clf = df['probable_cause_edit']\n",
        "    X = df[['year','month','operator_edit','county_edit','type_operation','source']]\n",
        "\n",
        "    # Preprocesamiento\n",
        "    num_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    cat_features = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "    preprocessor = ColumnTransformer(transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ])\n",
        "\n",
        "    X_processed = preprocessor.fit_transform(X)\n",
        "\n",
        "    le_causa = LabelEncoder()\n",
        "    y_clf_enc = le_causa.fit_transform(y_clf)\n",
        "    y_clf_oh = to_categorical(y_clf_enc)\n",
        "\n",
        "    X_train, X_test, y_reg_train, y_reg_test, y_clf_train, y_clf_test = train_test_split(\n",
        "        X_processed, y_reg, y_clf_oh, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    input_dim = X_train.shape[1]\n",
        "    num_classes = y_clf_oh.shape[1]\n",
        "    joblib.dump(le_causa, 'labelencoder_causa.pkl')\n",
        "    print(\"LabelEncoder guardado\")\n",
        "    print(f\"✅ Datos preparados: {X_train.shape[0]} muestras de entrenamiento\")\n",
        "\n",
        "    # AUTOMATIZACIÓN: Buscar y seleccionar automáticamente el mejor archivo\n",
        "    json_files = [f for f in os.listdir('.') if f.startswith('best_params_') and f.endswith('.json')]\n",
        "\n",
        "    if json_files:\n",
        "        print(f\"\\n🔍 Archivos de parámetros encontrados: {len(json_files)}\")\n",
        "\n",
        "        # ESTRATEGIA 1: Comparar todos los archivos y usar el mejor\n",
        "        best_file, best_params = load_and_compare_all_params(json_files)\n",
        "\n",
        "        if best_params:\n",
        "            print(f\"✅ Usando parámetros del archivo: {best_file}\")\n",
        "            model = build_model_from_params(best_params, input_dim, num_classes)\n",
        "        else:\n",
        "            print(\"❌ Error cargando parámetros, ejecutando nueva optimización...\")\n",
        "            json_filename, best_params = run_optuna_optimization(X_train, y_reg_train, y_clf_train, n_trials=150)\n",
        "            model = build_model_from_params(best_params, input_dim, num_classes)\n",
        "    else:\n",
        "        # PRIMERA VEZ: EJECUTAR OPTIMIZACIÓN\n",
        "        print(\"\\n🆕 No se encontraron archivos de parámetros - ejecutando optimización inicial...\")\n",
        "        json_filename, best_params = run_optuna_optimization(X_train, y_reg_train, y_clf_train, n_trials=150)\n",
        "        model = build_model_from_params(best_params, input_dim, num_classes)\n",
        "\n",
        "    print(\"\\n🏋️ Entrenando modelo final...\")\n",
        "\n",
        "    batch_size = best_params.get('batch_size', 64)\n",
        "\n",
        "    history = model.fit(\n",
        "        X_train,\n",
        "        {'regression': y_reg_train, 'classification': y_clf_train},\n",
        "        validation_split=0.2,\n",
        "        epochs=93,\n",
        "        batch_size=batch_size,\n",
        "        callbacks=[\n",
        "            EarlyStopping(patience=10, restore_best_weights=True),\n",
        "            ReduceLROnPlateau(patience=5, factor=0.5)\n",
        "        ],\n",
        "        verbose=1\n",
        "    )\n",
        "    # Guardar el modelo entrenado a disco justo aquí:\n",
        "    model.save(\"modelo_trained.h5\")\n",
        "    print(\"Modelo guardado en modelo_trained.h5\")\n",
        "    joblib.dump(preprocessor, 'preprocessor.pkl')\n",
        "    print(\"Preprocessor guardado en preprocessor.pkl\")\n",
        "    print(\"\\n🔬 Evaluando modelo...\")\n",
        "\n",
        "    test_results = model.evaluate(\n",
        "        X_test,\n",
        "        {'regression': y_reg_test, 'classification': y_clf_test},\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    predictions = model.predict(X_test, verbose=0)\n",
        "    y_reg_pred = predictions[0].flatten()\n",
        "    y_clf_pred = predictions[1]\n",
        "\n",
        "    r2 = r2_score(y_reg_test, y_reg_pred)\n",
        "    mse = mean_squared_error(y_reg_test, y_reg_pred)\n",
        "    mae = np.mean(np.abs(y_reg_test - y_reg_pred))\n",
        "\n",
        "    y_clf_pred_classes = np.argmax(y_clf_pred, axis=1)\n",
        "    y_clf_test_classes = np.argmax(y_clf_test, axis=1)\n",
        "    accuracy = accuracy_score(y_clf_test_classes, y_clf_pred_classes)\n",
        "    \n",
        "    # === NUEVAS SECCIONES: TABLAS PARA TODAS LAS MÉTRICAS ===\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"📊 TABLAS DETALLADAS DE MÉTRICAS POR ÉPOCA\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Obtener todas las métricas del historial\n",
        "    epochs_range = range(1, len(history.history['loss']) + 1)\n",
        "\n",
        "    # Función auxiliar para mostrar tabla\n",
        "    def show_metric_table(title, train_metric, val_metric, metric_name, show_diff=True):\n",
        "        print(f\"\\n{title}\")\n",
        "        print(\"-\" * 80)\n",
        "        if show_diff:\n",
        "            print(f\"{'Epoch':<8} {'Train '+metric_name:<15} {'Val '+metric_name:<15} {'Diferencia':<15}\")\n",
        "        else:\n",
        "            print(f\"{'Epoch':<8} {'Train '+metric_name:<15} {'Val '+metric_name:<15}\")\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        for i in range(0, len(train_metric), 5):\n",
        "            epoch = i + 1\n",
        "            train_val = train_metric[i]\n",
        "            val_val = val_metric[i]\n",
        "            if show_diff:\n",
        "                diff = abs(train_val - val_val)\n",
        "                print(f\"{epoch:<8} {train_val:<15.4f} {val_val:<15.4f} {diff:<15.4f}\")\n",
        "            else:\n",
        "                print(f\"{epoch:<8} {train_val:<15.4f} {val_val:<15.4f}\")\n",
        "\n",
        "        # Mostrar última época si no se mostró\n",
        "        if (len(train_metric) - 1) % 5 != 0:\n",
        "            i = len(train_metric) - 1\n",
        "            epoch = i + 1\n",
        "            train_val = train_metric[i]\n",
        "            val_val = val_metric[i]\n",
        "            if show_diff:\n",
        "                diff = abs(train_val - val_val)\n",
        "                print(f\"{epoch:<8} {train_val:<15.4f} {val_val:<15.4f} {diff:<15.4f}\")\n",
        "            else:\n",
        "                print(f\"{epoch:<8} {train_val:<15.4f} {val_val:<15.4f}\")\n",
        "\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        # Estadísticas de resumen\n",
        "        best_train = max(train_metric) if 'acc' in metric_name.lower() else min(train_metric)\n",
        "        best_val = max(val_metric) if 'acc' in metric_name.lower() else min(val_metric)\n",
        "        best_train_epoch = (train_metric.index(best_train) + 1) if 'acc' in metric_name.lower() else (train_metric.index(best_train) + 1)\n",
        "        best_val_epoch = (val_metric.index(best_val) + 1) if 'acc' in metric_name.lower() else (val_metric.index(best_val) + 1)\n",
        "\n",
        "        comparison = \"Mejor\" if 'acc' in metric_name.lower() else \"Menor\"\n",
        "        print(f\"\\nRESUMEN {metric_name.upper()}:\")\n",
        "        print(f\"{comparison} Train: {best_train:.4f} (Época {best_train_epoch})\")\n",
        "        print(f\"{comparison} Val:   {best_val:.4f} (Época {best_val_epoch})\")\n",
        "        print(f\"Final Train: {train_metric[-1]:.4f}\")\n",
        "        print(f\"Final Val:   {val_metric[-1]:.4f}\")\n",
        "\n",
        "    # 1. TABLA DE PÉRDIDA TOTAL\n",
        "    train_loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    show_metric_table(\"1. PÉRDIDA TOTAL vs ÉPOCAS\", train_loss, val_loss, \"Loss\")\n",
        "\n",
        "    # 2. TABLA DE ACCURACY\n",
        "    train_acc = history.history['classification_accuracy']\n",
        "    val_acc = history.history['val_classification_accuracy']\n",
        "    show_metric_table(\"2. ACCURACY vs ÉPOCAS\", train_acc, val_acc, \"Accuracy\")\n",
        "    print(f\"Accuracy en Test: {accuracy:.4f}\")\n",
        "\n",
        "    # 3. TABLA DE MAE REGRESIÓN\n",
        "    train_mae = history.history['regression_mae']\n",
        "    val_mae = history.history['val_regression_mae']\n",
        "    show_metric_table(\"3. MAE REGRESIÓN vs ÉPOCAS\", train_mae, val_mae, \"MAE\")\n",
        "    print(f\"MAE en Test: {mae:.4f}\")\n",
        "\n",
        "    # 4. TABLA DE PÉRDIDA CLASIFICACIÓN\n",
        "    train_cls_loss = history.history['classification_loss']\n",
        "    val_cls_loss = history.history['val_classification_loss']\n",
        "    show_metric_table(\"4. PÉRDIDA CLASIFICACIÓN vs ÉPOCAS\", train_cls_loss, val_cls_loss, \"Cls_Loss\")\n",
        "\n",
        "    # 5. TABLA DE PÉRDIDA REGRESIÓN\n",
        "    train_reg_loss = history.history['regression_loss']\n",
        "    val_reg_loss = history.history['val_regression_loss']\n",
        "    show_metric_table(\"5. PÉRDIDA REGRESIÓN vs ÉPOCAS\", train_reg_loss, val_reg_loss, \"Reg_Loss\")\n",
        "\n",
        "    # 6. TABLA CONSOLIDADA (RESUMEN)\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"6. TABLA CONSOLIDADA - TODAS LAS MÉTRICAS (cada 10 épocas)\")\n",
        "    print(\"=\"*100)\n",
        "    print(f\"{'Epoch':<8} {'Total_Loss':<12} {'Accuracy':<12} {'Reg_MAE':<12} {'Cls_Loss':<12} {'Reg_Loss':<12}\")\n",
        "    print(f\"{'':<8} {'T/V':<12} {'T/V':<12} {'T/V':<12} {'T/V':<12} {'T/V':<12}\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for i in range(0, len(train_loss), 10):\n",
        "        epoch = i + 1\n",
        "        print(f\"{epoch:<8} {train_loss[i]:<5.3f}/{val_loss[i]:<5.3f} {train_acc[i]:<5.3f}/{val_acc[i]:<5.3f} {train_mae[i]:<5.3f}/{val_mae[i]:<5.3f} {train_cls_loss[i]:<5.3f}/{val_cls_loss[i]:<5.3f} {train_reg_loss[i]:<5.3f}/{val_reg_loss[i]:<5.3f}\")\n",
        "\n",
        "    # Mostrar última época en tabla consolidada\n",
        "    if (len(train_loss) - 1) % 10 != 0:\n",
        "        i = len(train_loss) - 1\n",
        "        epoch = i + 1\n",
        "        print(f\"{epoch:<8} {train_loss[i]:<5.3f}/{val_loss[i]:<5.3f} {train_acc[i]:<5.3f}/{val_acc[i]:<5.3f} {train_mae[i]:<5.3f}/{val_mae[i]:<5.3f} {train_cls_loss[i]:<5.3f}/{val_cls_loss[i]:<5.3f} {train_reg_loss[i]:<5.3f}/{val_reg_loss[i]:<5.3f}\")\n",
        "\n",
        "    print(\"-\" * 100)\n",
        "    print(\"T = Training, V = Validation\")\n",
        "    \n",
        "\n",
        "    # === FIN DE NUEVAS SECCIONES ===\n",
        "\n",
        "    # Visualización (mantener las existentes)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.boxplot(df['log_release_prod_water_edit'].dropna())\n",
        "    plt.title('Boxplot of Logarithm of release_prod_water_edit')\n",
        "    plt.ylabel('Log(release_prod_water_edit)')\n",
        "    plt.grid(True)\n",
        "    print(\"Boxplot mostrado en pantalla\")\n",
        "    plt.show()\n",
        "\n",
        "    # Gráficos existentes + nuevo gráfico de accuracy\n",
        "    plt.figure(figsize=(16, 8))  # Aumentar tamaño para acomodar 4 subplots\n",
        "\n",
        "    plt.subplot(2, 3, 1)\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.title('Pérdida Total')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 3, 2)\n",
        "    plt.plot(history.history['regression_mae'], label='Training')\n",
        "    plt.plot(history.history['val_regression_mae'], label='Validation')\n",
        "    plt.title('MAE Regresión')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 3, 3)\n",
        "    plt.scatter(y_reg_test, y_reg_pred, alpha=0.6)\n",
        "    plt.plot([y_reg_test.min(), y_reg_test.max()], [y_reg_test.min(), y_reg_test.max()], 'r--')\n",
        "    plt.xlabel('Real')\n",
        "    plt.ylabel('Predicho')\n",
        "    plt.title(f'Regresión (R²={r2:.3f})')\n",
        "\n",
        "    # NUEVO: Gráfico de Accuracy vs Epochs\n",
        "    plt.subplot(2, 3, 4)\n",
        "    epochs_range = range(1, len(history.history['classification_accuracy']) + 1)\n",
        "    train_acc = history.history['classification_accuracy']\n",
        "    val_acc = history.history['val_classification_accuracy']\n",
        "    plt.plot(epochs_range, train_acc, label='Training', marker='o', markersize=2)\n",
        "    plt.plot(epochs_range, val_acc, label='Validation', marker='s', markersize=2)\n",
        "    plt.xlabel('Época')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Accuracy vs Épocas')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.subplot(2, 3, 5)\n",
        "    plt.plot(history.history['classification_loss'], label='Training')\n",
        "    plt.plot(history.history['val_classification_loss'], label='Validation')\n",
        "    plt.title('Pérdida Clasificación')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(2, 3, 6)\n",
        "    plt.plot(history.history['regression_loss'], label='Training')\n",
        "    plt.plot(history.history['val_regression_loss'], label='Validation')\n",
        "    plt.title('Pérdida Regresión')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    print(\"Gráfico de métricas\")\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"RESULTADOS FINALES\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Regresión:\")\n",
        "    print(f\"  R² Score: {r2:.4f}\")\n",
        "    print(f\"  MSE: {mse:.4f}\")\n",
        "    print(f\"  MAE: {mae:.4f}\")\n",
        "    print(f\"\\nClasificación:\")\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # === GUARDAR MÉTRICAS EN JSON CON TIMESTAMP ===\n",
        "    metrics_summary = {\n",
        "        \"timestamp\": pd.Timestamp.now().isoformat(),\n",
        "        \"metrics\": {\n",
        "            \"r2\": r2,\n",
        "            \"mse\": mse,\n",
        "            \"mae\": mae,\n",
        "            \"accuracy\": accuracy\n",
        "        },\n",
        "        \"model_params\": best_params,\n",
        "        \"training_epochs\": len(history.history['loss']),\n",
        "        \"final_training_loss\": float(history.history['loss'][-1]),\n",
        "        \"final_validation_loss\": float(history.history['val_loss'][-1])\n",
        "    }\n",
        "\n",
        "    print(\"\\nResumen final de métricas:\")\n",
        "    print(json.dumps(metrics_summary, indent=4))\n",
        "\n",
        "    # === MOSTRAR PREDICCIONES EN PANTALLA (no guardar) ===\n",
        "    reg_pred_original = np.expm1(y_reg_pred)\n",
        "\n",
        "    df_preds = pd.DataFrame({\n",
        "        'y_reg_real': np.expm1(y_reg_test.values),\n",
        "        'y_reg_pred': reg_pred_original,\n",
        "        'y_clf_real': le_causa.inverse_transform(y_clf_test_classes),\n",
        "        'y_clf_pred': le_causa.inverse_transform(y_clf_pred_classes)\n",
        "    })\n",
        "\n",
        "    print(\"\\nEjemplo de predicciones:\")\n",
        "    print(df_preds.head())\n",
        "\n",
        "    # === GUARDAR PARÁMETROS ACTUALIZADOS SI SE MEJORÓ ===\n",
        "    if 'best_file' in locals() and best_file:\n",
        "        # Comparar métricas actuales con las del archivo usado\n",
        "        try:\n",
        "            with open(best_file, 'r') as f:\n",
        "                old_data = json.load(f)\n",
        "\n",
        "            old_r2 = old_data.get('metrics', {}).get('r2', 0)\n",
        "            old_acc = old_data.get('metrics', {}).get('accuracy', 0)\n",
        "            old_score = old_r2 * 0.6 + old_acc * 0.4\n",
        "\n",
        "            current_score = r2 * 0.6 + accuracy * 0.4\n",
        "\n",
        "            if current_score > old_score:\n",
        "                new_filename = f\"best_params_improved_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "                with open(new_filename, 'w') as f:\n",
        "                    json.dump(metrics_summary, f, indent=4)\n",
        "                print(f\"✅ Métricas mejoradas! Guardado nuevo archivo: {new_filename}\")\n",
        "                print(f\"   Score anterior: {old_score:.4f} → Score actual: {current_score:.4f}\")\n",
        "            else:\n",
        "                print(f\"📊 Métricas actuales ({current_score:.4f}) no superaron las del archivo usado ({old_score:.4f})\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Error comparando métricas: {e}\")\n",
        "\n",
        "    print(\"\\n🎉 Workflow completado automáticamente!\")\n",
        "\n",
        "    return model, best_params\n",
        "\n",
        "# Ejecutar workflow\n",
        "main_workflow()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Carga del CSV con separador y codificación específica\n",
        "df = pd.read_csv('spill_data_cleaned.csv', sep=';', encoding='latin1')\n",
        "\n",
        "# Lista de columnas de interés\n",
        "columnas = ['operator_edit', 'county_edit', 'type_operation', 'source']\n",
        "\n",
        "# Mostrar valores únicos para cada columna\n",
        "for col in columnas:\n",
        "    print(f\"\\nValores únicos de '{col}':\")\n",
        "    print(df[col].unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Valores únicos de 'county_edit':\")\n",
        "print(df['county_edit'].dropna().unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nValores únicos de 'type_operation':\")\n",
        "print(df['type_operation'].dropna().unique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nValores únicos de 'source':\")\n",
        "print(df['source'].dropna().unique())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
